import api from './index'
import axios from 'axios'
import { appConfig } from '@/config'

// ç³»ç»Ÿæç¤ºç±»å‹
export interface SystemPrompt {
  role: string
  content: string
}

// æ¨¡å‹ä¿¡æ¯æ¥å£
export interface ModelInfo {
  id: string
  name: string
  provider: string
  description?: string | null
  maxTokens?: number | null
  available: boolean
  pricePerToken?: number | null
}

// è·å–æ¨¡å‹è¯·æ±‚æ¥å£
export interface GetModelsRequest {
  apiUrl: string
  apiKey: string
  api?: string
}

// è¯·æ±‚å‚æ•°æ¥å£
export interface ChatRequest {
  apiUrl: string
  apiKey: string
  api: string
  model: string
  message: string
  temperature?: number
  maxTokens?: number
  systemPrompts: SystemPrompt[]
  chatMessages?: Message[]
}

// ä½¿ç”¨æƒ…å†µæ¥å£
export interface Usage {
  prompt_tokens: number
  completion_tokens: number
  total_tokens: number
}

// æ¶ˆæ¯æ¥å£
export interface Message {
  role: string
  content: string
}

// é€‰é¡¹æ¥å£
export interface Choice {
  message: Message
  finish_reason: string
  index: number
}

// å…ƒæ•°æ®æ¥å£
export interface Metadata {
  id: string
  object: string
  created: number
  model: string
  usage: Usage
  choices: Choice[]
}

// å“åº”æ¥å£
export interface ChatResponse {
  content: string
  model: string
  tokenCount: number
  responseTime: number
  success: boolean
  errorMessage?: string | null
  metadata: Metadata
}

// æ—¥å¿—å·¥å…·å‡½æ•°
const logChatRequest = (request: ChatRequest) => {
  console.group('ğŸš€ å‘é€èŠå¤©è¯·æ±‚')
  console.log('æ—¶é—´:', new Date().toLocaleString())
  console.log('API URL:', request.apiUrl)
  console.log('API ç±»å‹:', request.api)
  console.log('API Key:', request.apiKey)
  console.log('æ¨¡å‹:', request.model)
  console.log('æ¶ˆæ¯å†…å®¹:', request.message)
  console.log('ç³»ç»Ÿæç¤ºè¯:', request.systemPrompts)
  console.log('å†å²æ¶ˆæ¯:', request.chatMessages ? JSON.stringify(request.chatMessages) : 'æ— ')
  console.log('æ¸©åº¦:', request.temperature)
  console.log('æœ€å¤§Token:', request.maxTokens)
  console.log('å®Œæ•´è¯·æ±‚æ•°æ®:', JSON.stringify(request))
  console.groupEnd()
}

const logChatResponse = (response: ChatResponse) => {
  console.group('ğŸ“¨ æ”¶åˆ°AIå›å¤')
  console.log('æ—¶é—´:', new Date().toLocaleString())
  console.log('çŠ¶æ€:', response.success ? 'æˆåŠŸ' : 'å¤±è´¥')
  console.log('æ¨¡å‹:', response.model)
  console.log('Tokenæ•°é‡:', response.tokenCount)
  console.log('å“åº”æ—¶é—´:', response.responseTime + 'ms')
  console.log('å›å¤å†…å®¹:', response.content)
  if (response.metadata) {
    console.group('å…ƒæ•°æ®')
    console.log('ID:', response.metadata.id)
    console.log('åˆ›å»ºæ—¶é—´:', new Date(response.metadata.created * 1000).toLocaleString())
    console.log('ä½¿ç”¨æƒ…å†µ:', {
      æç¤ºè¯Token: response.metadata.usage.prompt_tokens,
      å›å¤Token: response.metadata.usage.completion_tokens,
      æ€»Token: response.metadata.usage.total_tokens
    })
    console.groupEnd()
  }
  if (!response.success) {
    console.error('é”™è¯¯ä¿¡æ¯:', response.errorMessage)
  }
  console.groupEnd()
}

const logError = (error: Error, context: string) => {
  console.group('âŒ é”™è¯¯')
  console.log('æ—¶é—´:', new Date().toLocaleString())
  console.log('ä¸Šä¸‹æ–‡:', context)
  console.error('é”™è¯¯ä¿¡æ¯:', error.message)
  console.error('é”™è¯¯å †æ ˆ:', error.stack)
  console.groupEnd()
}

/**
 * å‘é€èŠå¤©è¯·æ±‚åˆ°LLM
 * @param data èŠå¤©è¯·æ±‚å‚æ•°
 * @returns èŠå¤©å“åº”
 */
export const sendChatRequest = async (data: ChatRequest): Promise<ChatResponse> => {
  try {
    const startTime = Date.now()
    // ä½¿ç”¨å›ºå®šçš„APIæ¥å£URL
    const url = `${appConfig.api.baseUrl}/api/llm/chat`

    // å‡†å¤‡è¯·æ±‚æ•°æ®
    const requestData = {
      ...data,
      // ç¡®ä¿å†å²æ¶ˆæ¯å­—æ®µå­˜åœ¨å¹¶æ­£ç¡®æ ¼å¼åŒ–
      messages: data.chatMessages || []
    };

    // æ‰“å°è¯·æ±‚ä¿¡æ¯
    logChatRequest(data)
    console.log('å‘é€åˆ°åç«¯çš„å®é™…æ•°æ®:', JSON.stringify(requestData))

    // å‘é€è¯·æ±‚
    const response = await fetch(url, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(requestData),
    })

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`)
    }

    const result = await response.json() as ChatResponse
    result.responseTime = Date.now() - startTime

    // æ‰“å°å“åº”ä¿¡æ¯
    logChatResponse(result)

    return result
  } catch (error) {
    // æ‰“å°é”™è¯¯ä¿¡æ¯
    logError(error instanceof Error ? error : new Error(String(error)), 'sendChatRequest')
    throw error
  }
}

/**
 * è·å–å¯ç”¨çš„LLMæ¨¡å‹åˆ—è¡¨
 * @param data è¯·æ±‚å‚æ•°
 * @returns æ¨¡å‹åˆ—è¡¨
 */
export const getAvailableModels = async (data: GetModelsRequest): Promise<ModelInfo[]> => {
  try {
    // ä½¿ç”¨å›ºå®šçš„APIæ¥å£URL
    const url = `${appConfig.api.baseUrl}/api/llm/models`

    console.group('ğŸ” è·å–å¯ç”¨æ¨¡å‹')
    console.log('æ—¶é—´:', new Date().toLocaleString())
    console.log('API URL:', data.apiUrl)
    console.log('API ç±»å‹:', data.api || 'openai')
    console.log('API Key:', data.apiKey)
    console.groupEnd()

    const response = await fetch(url, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(data),
    })

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`)
    }

    const models = await response.json() as ModelInfo[]

    console.group('ğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨')
    console.log('æ—¶é—´:', new Date().toLocaleString())
    console.log('æ¨¡å‹æ•°é‡:', models.length)
    console.table(models)
    console.groupEnd()

    return models
  } catch (error) {
    logError(error instanceof Error ? error : new Error(String(error)), 'getAvailableModels')
    throw error
  }
}

// åˆ›å»ºé»˜è®¤çš„èŠå¤©è¯·æ±‚é…ç½®
export const createDefaultChatRequest = (
  message: string,
  apiUrl: string = 'https://api.openai.com/v1/chat/completions',
  apiKey: string = '',
  api: string = 'openai',
  model: string = 'gpt-4',
  temperature: number = 0.7,
  maxTokens: number = 2000,
  customSystemPrompts?: SystemPrompt[]
): ChatRequest => {
  return {
    apiUrl,
    apiKey,
    api,
    model,
    message,
    temperature,
    maxTokens,
    systemPrompts: customSystemPrompts || [
      {
        role: 'system',
        content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”é—®é¢˜ã€‚',
      },
    ],
  }
}

// åˆ›å»ºé»˜è®¤çš„æ¨¡å‹è¯·æ±‚é…ç½®
export const createDefaultModelsRequest = (apiKey: string, api: string = 'openai', apiUrl: string = 'https://api.openai.com/v1'): GetModelsRequest => {
  return {
    apiUrl,
    apiKey,
    api
  }
}

// ç¤ºä¾‹ï¼šä½¿ç”¨è‡ªå®šä¹‰APIåˆ›å»ºèŠå¤©è¯·æ±‚
export const createCustomChatRequest = (
  message: string,
  customSystemPrompts?: SystemPrompt[]
): ChatRequest => {
  return createDefaultChatRequest(
    message,
    'https://chatgtp.vin',
    'sk-P2pSLjuCWtHZEU78nfPGCkbZtgesZppuVonLeM9Lms7WImyO',
    'openai_compatible',
    'deepseek-r1',
    0.7,
    2000,
    customSystemPrompts
  )
}

// é»˜è®¤å¯¼å‡ºæ‰€æœ‰å†…å®¹
export default {
  sendChatRequest,
  getAvailableModels,
  createDefaultChatRequest,
  createDefaultModelsRequest,
  createCustomChatRequest
}
